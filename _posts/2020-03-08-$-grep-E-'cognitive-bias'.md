
<center><img src="/images/18-cognitive-bias-examples.jpg" alt="Photo courtesy of https://venitism.wordpress.com/2018/03/04/cognitive-bias-3/"></center>

<div style="text-align: center;">

   <p>Read Time: 2 minutes 56 seconds</p>
    
<article class="post detailed">
   
            $ grep -E 'cognitive bias'
  
  <div style="text-align: left;">
<p>There’s a specific question that is asked of industry peers in varying degrees – one I am guilty of asking as well – is <em>“how do I get started?”</em></p>

<p>At this stage of my career, I would offer the following:</p>

<p>Acknowledge to yourself that no matter how smart or skilled you think you are, the quality of your work result is directly connected to mindset and. No reasonable professional can believe that they are infallible of cognitive bias. The sooner you accept this, the better you will become.</p>

<p>The United States Central Intelligence Agency defines Cognitive Bias as <em>“mental errors caused by our simplified information processing strategies.”</em> The agency continues to explain that, <em>“..it is important to distinguish cognitive biases from other forms of bias, such as cultural bias, organizational bias, or bias that results from one's self-interest. In other words, a cognitive bias does not result from any emotional or intellectual predisposition toward a certain judgment, but rather from subconscious mental procedures for processing information. A cognitive bias is a mental error that is consistent and predictable.”</em></p>

<p>To state it concisely within the context of cybersecurity: Cognitive bias is an inherent preference for one choice over another on the act of the professional conducting the analysis. An interdependence of this concept is the mindset of the professional themselves; this defines preference despite its risk of inhibiting the holistic cognitive process altogether. These two concepts CONSTRUCT reality for the decision-makers instead of recording it.</p>

<p>Not discussing this more is dangerous to yourself, your team, and your organization due to its propensity to introduce flawed intelligence caused by flawed analysis.</p>

<p><em>”So this is where we get our ROI on the machine learning and artificial intelligence tools we invested in, right?”</em> #datELKdoe</p>

<center><img src="/images/ShiningLaugh.gif"></center>

<p>Let me mirror that back to you. Removing the risk of human bias with decision-making software written by humans. That is. Well. Iconic.</p>

<p>Back to bias.</p>

<p>In no particular order, the different types of bias and potential consequences of each within the context of cybersecurity investigations are as follows:</p>

   <p>1. Anchoring: Relying too heavily (“anchoring”) on a trait or piece of information when making a decision.</p>
        <p>Consequence: Human cognition struggles to move away from the initial assessment and consider additional information.</p>
    <p>2. Absence of Evidence: Consideration of another hypothesis due to lack of evidence for the first. (I have also seen this conceptualized as “if I don’t see it, then it doesn’t exist)</p>
        Consequence: Fitting the evidence to a hypothesis, rather than developing a hypothesis that fits the evidence.
    <p>3. Confirmation: Looking for evidence that points to the expected answer and ignoring (or undervaluing) evidence that contradicts it. (Otherwise known as “cherry-picking”)</p>
        <p>Consequence: Creates a “tunnel vision” for the analyst and avoids contradictory information.</p>
    <p>4. Intuition: Cognitive shortcuts, influenced by emotions and experiences, used even when the evidence does not support them.</p>
        <p>Consequence: Gives a greater emphasis to patters that may not exist than the evidence supports.</p>
    <p>5. Framing: Using surrounding circumstances to interpret the information, even if unwarranted. (An example to this that I’ve heard are beer commercials targeting age demographics by implying that drinking their beer will help them get beautiful women).</p>
        <p>Consequence: If the “frame” is artificial or inappropriate to the investigation, it can distort the understanding of the data.</p>
        <p>See my earlier comment re: AI as a possible counterbalance.</p>
    <p>6. Representativeness: Judging whether something (someone) belongs to a specific category, and using stereotypes to predict what will happen.</p>
        <p>Consequence: This often results in inaccurate prototypes or stereotypes, which then become part of the analysis.</p>
    <p>7. Misrepresenting Physical Evidence: Unintentionally misinterpreting physical evidence.</p>
        <p>Consequence: Finding support (or additional support) that does not exist.</p>
    <p>8. Motivational: Unintentionally concluding or seeing evidence a certain way due to internal motivation. (This can be seen demonstrated in professionals that overstate informational value or withhold information for the intention of looking valuable to an investigation or organization.)</p>
        <p>Consequence: This can reinforce unsupported beliefs (of an individual or group) and cause systematic mistakes.</p>
        <p>2. See <a href="https://www.nytimes.com/2018/11/12/opinion/russia-meddling-disinformation-fake-news-elections.html" title="Operation Infektion">as a weaponized method to exploit this specific bias.</p>
    <p>9. Assimilation: Using preexisting belief structures to evaluate new evidence or information. (<em>“Everyone is/thinks like me”</em>)</p>
        <p>Consequence: Risks the tendency to fall into the trap of “group think”</p>

<p>So now that I have rudely invited myself to the party and pointed out the turd in your punch bowl, what now?</p>

<p>More soon.</p>
